{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rasterio\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import box\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "from rasterio import features\n",
    "\n",
    "from rasterio.features import rasterize\n",
    "from rasterio.transform import from_origin\n",
    "import pandas as pd\n",
    "from rasterio import features\n",
    "from shapely.geometry import Polygon, MultiPolygon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCTION \n",
    "\n",
    "The raster_to_polygons_optimized function is designed to convert a raster dataset into a GeoDataFrame of polygons, where each polygon represents an individual cell from the raster. \n",
    "\n",
    "The function takes a single argument, raster_path, which specifies the file path of the raster to be processed. It begins by opening the raster file using rasterio, extracting the transformation parameters, reading the first band of data, and identifying the no-data value specified in the raster's metadata. \n",
    "\n",
    "The function then identifies all valid cells—those not equal to the no-data value—and iterates over these cells. For each valid cell, it calculates the polygon geometry that corresponds to the cell's spatial extent using the raster's transformation parameters and assigns the cell's value to this polygon. \n",
    "\n",
    "These polygons and their associated values are then aggregated into a GeoDataFrame, which is assigned the same Coordinate Reference System (CRS) as the source raster. \n",
    "\n",
    "The resulting GeoDataFrame, containing a 'value' column for cell values and a 'geometry' column for the corresponding polygons, is returned by the function. This optimized approach ensures efficient processing by focusing only on valid raster cells, thereby excluding areas with no-data values and reducing computational overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raster_to_polygons_optimized(raster_path):\n",
    "    \"\"\"\n",
    "    Convert a raster dataset to a GeoDataFrame where each valid cell (non-no-data) is represented as a polygon.\n",
    "\n",
    "    This function opens a raster file, reads its first band, and converts each valid cell into a polygon. The value\n",
    "    of each cell is retained and associated with the corresponding polygon. The resulting GeoDataFrame contains\n",
    "    polygons with their associated values and is set to the same CRS as the input raster.\n",
    "\n",
    "    Parameters:\n",
    "    - raster_path (str): The file path of the raster to be processed.\n",
    "\n",
    "    Returns:\n",
    "    - gpd.GeoDataFrame: A GeoDataFrame with two columns: 'value' containing the cell values and 'geometry'\n",
    "      containing the corresponding polygon geometries for each valid raster cell.\n",
    "    \"\"\"\n",
    "    # Extract the base filename without extension to use as the column name\n",
    "    filename = os.path.splitext(os.path.basename(raster_path))[0]\n",
    "\n",
    "    # Open the raster file using rasterio\n",
    "    with rasterio.open(raster_path) as src:\n",
    "        \n",
    "        # Extract the affine transform for the raster\n",
    "        transform = src.transform\n",
    "        # Read the first band of the raster\n",
    "        data = src.read(1)\n",
    "        # Retrieve the no-data value set in the raster's metadata\n",
    "        nodata = src.nodata\n",
    "\n",
    "    # Identify valid cells (cells not equal to the no-data value)\n",
    "    valid_cells = np.argwhere(data != nodata)\n",
    "\n",
    "    polygons = []  # List to store polygon geometries\n",
    "    values = []  # List to store the values of the valid cells\n",
    "\n",
    "    # Iterate over the indices of valid cells to create polygons\n",
    "    for (j, i) in valid_cells:\n",
    "        # Extract the value of the current cell\n",
    "        value = data[j, i]\n",
    "        # Create a polygon geometry for the current cell based on its coordinates and the raster's transform\n",
    "        polygon = box(\n",
    "            transform[2] + i * transform[0],  # Minx\n",
    "            transform[5] + (j + 1) * transform[4],  # Miny\n",
    "            transform[2] + (i + 1) * transform[0],  # Maxx\n",
    "            transform[5] + j * transform[4]  # Maxy\n",
    "        )\n",
    "        # Append the current polygon and its value to their respective lists\n",
    "        polygons.append(polygon)\n",
    "        values.append(value)\n",
    "\n",
    "    # Create a GeoDataFrame with the polygons and their associated values\n",
    "    gdf = gpd.GeoDataFrame({filename: values, 'geometry': polygons})\n",
    "    # Set the CRS of the GeoDataFrame to match the CRS of the source raster\n",
    "    gdf.crs = src.crs\n",
    "\n",
    "    return gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gdf_test = raster_to_polygons_optimized()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A raster transform is a set of coefficients that define the spatial relationship between the coordinates of pixels in a raster dataset and the geographic coordinates on the surface of the Earth. This transform allows you to map the row and column indices of pixels in the raster grid to their corresponding geographic locations (such as latitude and longitude or coordinates in a projected coordinate system).\n",
    "\n",
    "The raster transform typically includes six coefficients, often represented in an affine transformation matrix. These coefficients are:\n",
    "\n",
    "a: The width of a pixel in the units of the coordinate system (e.g., meters).\n",
    "b and d: These coefficients typically rotate the raster, but in most north-up images, these are zero.\n",
    "c: The X-coordinate of the upper-left corner of the upper-left pixel.\n",
    "e: The height of a pixel in the units of the coordinate system, which is usually a negative value because pixel row indices increase downward, while geographic coordinates usually increase upward.\n",
    "f: The Y-coordinate of the upper-left corner of the upper-left pixel.\n",
    "In the context of the rasterio library, the transform is often used to calculate the geographic coordinates of the center of a pixel given its row and column indices, or vice versa. The transform object can perform these calculations through methods like * (to convert pixel coordinates to geographic coordinates) and ~ (to convert geographic coordinates to pixel coordinates).\n",
    "\n",
    "An example affine transform could look like this: [a, b, c, d, e, f], where a, d, and e are the pixel sizes and rotation terms, and c and f give the geographic coordinates of the upper-left pixel.\n",
    "\n",
    "Understanding the raster transform is crucial for tasks like georeferencing, where you need to align the raster data with a specific geographic location, or when performing spatial analyses that require the conversion of pixel coordinates to real-world geographic locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Raster(s) and Create 'INPUT REPORTING UNITS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest resolution: (196.0, 196.0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Folder containing all your raster files\n",
    "raster_folder = r\"C:\\Users\\bsf31\\Documents\\post-meds\\data\\signal\\riparian_restoration\"\n",
    "# Get a list of all raster files in the folder (assuming they are TIFF files)\n",
    "raster_files = [f for f in os.listdir(raster_folder) if f.endswith('.tif')]\n",
    "\n",
    "# Initialize variables to store the smallest resolution\n",
    "smallest_res = None\n",
    "\n",
    "# Iterate through each raster file to find the smallest resolution\n",
    "for raster_file in raster_files:\n",
    "    with rasterio.open(os.path.join(raster_folder, raster_file)) as src:\n",
    "        # src.res returns a tuple (width, height) of pixels\n",
    "        if smallest_res is None or (src.res[0] < smallest_res[0] and src.res[1] < smallest_res[1]):\n",
    "            smallest_res = src.res\n",
    "\n",
    "# smallest_res now contains the smallest resolution by width and height\n",
    "print(\"Smallest resolution:\", smallest_res)\n",
    "\n",
    "# Initialize an empty GeoDataFrame to store the combined results\n",
    "combined_gdf = gpd.GeoDataFrame()\n",
    "\n",
    "# Process each raster file\n",
    "for raster_file in raster_files:\n",
    "    raster_path = os.path.join(raster_folder, raster_file)\n",
    "    with rasterio.open(raster_path) as src:\n",
    "        # Check if the raster's resolution matches the most common resolution\n",
    "        if src.res != smallest_res:\n",
    "            print(f\"Skipping {raster_file} due to mismatching resolution.\")\n",
    "            continue  # Skip this file\n",
    "\n",
    "    # If resolution matches, process the raster\n",
    "    current_gdf = raster_to_polygons_optimized(raster_path)\n",
    "    \n",
    "    # Merge the current GeoDataFrame with the combined one\n",
    "    if combined_gdf.empty:\n",
    "        combined_gdf = current_gdf\n",
    "    else:\n",
    "        combined_gdf = combined_gdf.merge(current_gdf, on='geometry', how='outer')\n",
    "\n",
    "# Save the combined GeoDataFrame to a GeoPackage\n",
    "output_gpkg_path = r'C:\\Users\\bsf31\\Documents\\post-meds\\data\\signal\\input_reporting_units\\input_reporting_units.gpkg'\n",
    "combined_gdf.to_file(output_gpkg_path, layer='WHRHabitatType', driver='GPKG')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\bsf31\\\\Documents\\\\post-meds\\\\data\\\\signal\\\\input_reporting_units\\\\input_reporting_units.gpkg'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_gpkg_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional - Append New Single File Raster Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the existing GeoPackage and the new raster file\n",
    "existing_gpkg_path = output_gpkg_path\n",
    "# Load the existing GeoPackage into a GeoDataFrame\n",
    "existing_gdf = gpd.read_file(existing_gpkg_path, layer='combined_layer')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_raster_path = 'path_to_new_raster_file.tif'\n",
    "# Process the new raster to a GeoDataFrame\n",
    "new_gdf = raster_to_polygons_optimized(new_raster_path)\n",
    "\n",
    "# Merge the new GeoDataFrame with the existing one\n",
    "# Ensure the merge is done based on geometry or another suitable common attribute\n",
    "updated_gdf = existing_gdf.merge(new_gdf, on='geometry', how='outer')\n",
    "\n",
    "# Update the GeoPackage with the updated GeoDataFrame\n",
    "# This example overwrites the existing layer; you could also choose to add a new layer\n",
    "updated_gdf.to_file(existing_gpkg_path, layer='combined_layer', driver='GPKG')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_path = r\"C:\\Users\\bsf31\\Documents\\post-meds\\data\\signal\\RWMP_AREA_2229.gpkg\"\n",
    "study_area = gpd.read_file(study_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REFERENCE RASTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_crs = 'EPSG:2229'\n",
    "output_resolution = (33.75336306985997936, -34.37544019661349637)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5807047.11341745, 1961361.28632066, 6127434.03567656,\n",
       "       2040906.05493562])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study_area_bounds =study_area.total_bounds\n",
    "study_area_bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector to Raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_to_raster(input_vector, output_raster, attribute, study_area_bounds, value_mapping, single_value=None, resolution=(abs(0.00026949458523585647), abs(-0.00026949458523585647)), dtype='uint16'):\n",
    "    # Check if input_vector is a GeoDataFrame or a file path\n",
    "    if isinstance(input_vector, gpd.GeoDataFrame):\n",
    "        gdf = input_vector\n",
    "    else:\n",
    "        # Read the input vector file (GeoPackage or Shapefile) into a GeoDataFrame\n",
    "        gdf = gpd.read_file(input_vector)\n",
    "    # Reproject the GeoDataFrame to the desired CRS (EPSG:2229)\n",
    "    gdf = gdf.to_crs(epsg=2229)\n",
    "\n",
    "    # Ensure that categorical column is string\n",
    "    gdf[attribute] = gdf[attribute].astype(str)\n",
    "   \n",
    "\n",
    "   # If single_value is None, convert the attribute column to numerical values\n",
    "    # If single_value is provided, set the attribute column to the provided single_value\n",
    "    if single_value is None:\n",
    "        gdf[attribute] = gdf[attribute].replace(value_mapping).astype(dtype)\n",
    "    else:\n",
    "        gdf[attribute] = single_value\n",
    "\n",
    "\n",
    "    # Use the study area bounds to define the dimensions and transform of the output raster\n",
    "    minx, miny, maxx, maxy = study_area_bounds\n",
    "    width = 9492\n",
    "    height = 2314\n",
    "    out_transform = rasterio.transform.from_bounds(minx, miny, maxx, maxy, width, height)\n",
    "\n",
    "\n",
    "    # Define the metadata for the output raster file\n",
    "    out_meta = {\n",
    "        'driver': 'GTiff',\n",
    "        'width': width,\n",
    "        'height': height,\n",
    "        'count': 1,\n",
    "        'dtype': dtype,\n",
    "        'crs': 'EPSG:2229',\n",
    "        'transform': out_transform,\n",
    "        'nodata': 0\n",
    "    }\n",
    "    \n",
    "    # Open the output raster file for writing with the specified metadata\n",
    "    with rasterio.open(output_raster, 'w', **out_meta) as dst:\n",
    "        # Create a generator of tuples containing the geometry and attribute value for each feature in the input vector data\n",
    "        shapes = ((geom, value) for geom, value in zip(gdf['geometry'], gdf[attribute]))\n",
    "        \n",
    "        # Burn the geometries and their corresponding attribute values into a raster array\n",
    "        burned = features.rasterize(\n",
    "            shapes=shapes,         # The generator of geometry-attribute tuples\n",
    "            fill=0,                # The default value for pixels not covered by any geometry\n",
    "            out_shape=(height, width), # The shape of the output raster array (number of rows and columns)\n",
    "            transform=out_transform,   # The affine transformation matrix that maps pixel coordinates to the coordinate reference system\n",
    "            dtype=dtype            # The data type of the raster array\n",
    "        )\n",
    "        \n",
    "        # Write the burned raster array to the output raster file\n",
    "        dst.write(burned, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to call Vector to Raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To write text file of value mapping\n",
    "def write_value_mapping(value_mapping, output_file):\n",
    "    with open(output_file, 'w') as f:\n",
    "        for key, value in value_mapping.items():\n",
    "            f.write(f'{key}: {value}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_columns(input_vector, output_dir, study_area_bounds, resolution, columns, value_mapping, single_value=None, file_name=None):\n",
    "    if file_name is None:\n",
    "        file_name = os.path.splitext(os.path.basename(input_vector))[0]\n",
    "\n",
    "    for column in columns:\n",
    "        column_output_dir = os.path.join(output_dir, column)\n",
    "        os.makedirs(column_output_dir, exist_ok=True)\n",
    "\n",
    "        output_raster = f\"{column_output_dir}/{file_name}_{column}_raster.tif\"\n",
    "        vector_to_raster(input_vector, output_raster, column, study_area_bounds, value_mapping[column], single_value=single_value, resolution=resolution)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files_from_list(file_list, output_dir, study_area_bounds, resolution, columns, single_value=None):\n",
    "    # Generate a global value mapping from the list of unique values for each column\n",
    "    global_value_mapping = {column: set() for column in columns}\n",
    "    \n",
    "    for file_path in file_list:\n",
    "        gdf = gpd.read_file(file_path)\n",
    "        for column in columns:\n",
    "            global_value_mapping[column].update(gdf[column].astype(str).unique())\n",
    "    \n",
    "    for column in columns:\n",
    "        global_value_mapping[column] = {value: idx for idx, value in enumerate(sorted(list(global_value_mapping[column])), 1)}\n",
    "        print(f\"Global value mapping for {column}:\")\n",
    "        for value in global_value_mapping[column]:\n",
    "            print(f\"{value}: {global_value_mapping[column][value]}\")\n",
    "    \n",
    "    # Write the global value mapping for each column to separate .txt files in the corresponding column folder\n",
    "    for column in columns:\n",
    "        column_output_dir = os.path.join(output_dir, column)\n",
    "        os.makedirs(column_output_dir, exist_ok=True)\n",
    "\n",
    "        output_value_mapping_file = f\"{column_output_dir}/{column}_global_value_mapping.txt\"\n",
    "        write_value_mapping(global_value_mapping[column], output_value_mapping_file)\n",
    "\n",
    "    # Process each file with the global value mapping\n",
    "    for file_path in file_list:\n",
    "        file_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        process_columns(file_path, output_dir, study_area_bounds, columns=columns, resolution=resolution, value_mapping=global_value_mapping, single_value=single_value, file_name=file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where files will save, can add subfolders if desired\n",
    "output_dir = os.path.join(r\"C:\\Users\\bsf31\\Documents\\post-meds\\data\\signal\\test_vector\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "test_vector = [os.path.join(output_dir, 'calveg_cwhr_rwmp.gpkg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global value mapping for WHRHabitatType:\n",
      "Annual Grassland: 1\n",
      "Barren: 2\n",
      "Blue Oak Woodland: 3\n",
      "Blue Oak-Foothill Pine: 4\n",
      "Chamise-Redshank Chaparral: 5\n",
      "Closed-Cone Pine-Cypress: 6\n",
      "Coastal Oak Woodland: 7\n",
      "Coastal Scrub: 8\n",
      "Cropland: 9\n",
      "Deciduous Orchard: 10\n",
      "Desert Wash: 11\n",
      "Eucalyptus: 12\n",
      "Evergreen Orchard: 13\n",
      "Freshwater Emergent Wetland: 14\n",
      "Lacustrine: 15\n",
      "Mixed Chaparral: 16\n",
      "Montane Hardwood: 17\n",
      "Montane Hardwood-Conifer: 18\n",
      "Montane Riparian: 19\n",
      "Pasture: 20\n",
      "Perennial Grassland: 21\n",
      "Riverine: 22\n",
      "Saline Emergent Wetland: 23\n",
      "Sierran Mixed Conifer: 24\n",
      "Urban: 25\n",
      "Valley Oak Woodland: 26\n",
      "Valley-Foothill Riparian: 27\n",
      "Vineyard: 28\n",
      "Water: 29\n"
     ]
    }
   ],
   "source": [
    "process_files_from_list(test_vector, output_dir, study_area_bounds, output_resolution, columns=['WHRHabitatType'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Append Vector Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_raster_path = r\"C:\\Users\\bsf31\\Documents\\post-meds\\data\\signal\\test_vector\\WHRHabitatType\\calveg_cwhr_rwmp_WHRHabitatType_raster.tif\"\n",
    "# Process the new raster to a GeoDataFrame\n",
    "new_gdf = raster_to_polygons_optimized(new_raster_path)\n",
    "\n",
    "# Merge the new GeoDataFrame with the existing one\n",
    "# Ensure the merge is done based on geometry or another suitable common attribute\n",
    "updated_gdf = existing_gdf.merge(new_gdf, on='geometry', how='outer')\n",
    "\n",
    "# Update the GeoPackage with the updated GeoDataFrame\n",
    "# This example overwrites the existing layer; you could also choose to add a new layer\n",
    "updated_gdf.to_file(existing_gpkg_path, layer='new_combined_layer', driver='GPKG')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_vector_to_gpkg(vector_path, gpkg_path, input_layer, output_layer, dissolve=False, aggfunc=None):\n",
    "    \"\"\"\n",
    "    Appends data from a vector file to an existing layer in a GeoPackage and optionally aggregates the result,\n",
    "    saving it as a new layer within the same GeoPackage.\n",
    "\n",
    "    Parameters:\n",
    "    - vector_path (str): The file path of the vector file to be appended.\n",
    "    - gpkg_path (str): The file path of the GeoPackage to which the data will be appended.\n",
    "    - input_layer (str): The name of the existing layer in the GeoPackage to which the vector data will be appended.\n",
    "    - output_layer (str): The name of the new layer in the GeoPackage where the result will be saved.\n",
    "    - dissolve (bool, optional): Whether to dissolve (aggregate) the spatial join results. Defaults to False.\n",
    "    - aggfunc (str or dict, optional): The aggregation function to use when dissolving. This parameter is only used if dissolve is True. Defaults to None.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the existing GeoPackage layer\n",
    "    gdf_units = gpd.read_file(gpkg_path, layer=input_layer)\n",
    "\n",
    "    # Load the vector file\n",
    "    gdf_vector = gpd.read_file(vector_path)\n",
    "\n",
    "    # Ensure the CRS of the vector data matches that of the GeoPackage layer\n",
    "    if gdf_units.crs != gdf_vector.crs:\n",
    "        gdf_vector = gdf_vector.to_crs(gdf_units.crs)\n",
    "\n",
    "    # Perform a spatial join between the GeoPackage layer and the vector data\n",
    "    gdf_joined = gpd.sjoin(gdf_units, gdf_vector, how=\"left\", op=\"intersects\")\n",
    "\n",
    "    # Check if dissolving (aggregation) is required\n",
    "    if dissolve:\n",
    "        # If no specific aggregation function is provided, raise an error\n",
    "        if aggfunc is None:\n",
    "            raise ValueError(\"An aggregation function must be provided when dissolve is True.\")\n",
    "        \n",
    "        # Dissolve (aggregate) the joined data\n",
    "        gdf_result = gdf_joined.dissolve(by=\"index_left\", aggfunc=aggfunc)\n",
    "    else:\n",
    "        # If dissolving is not required, the result is the spatially joined data\n",
    "        gdf_result = gdf_joined\n",
    "\n",
    "    # Save the result as a new layer in the GeoPackage\n",
    "    gdf_result.to_file(gpkg_path, layer=output_layer, driver='GPKG')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " GeoPandas, when you use the `dissolve` method, the `aggfunc` parameter specifies how to aggregate data for columns other than the one used for grouping (in this case, the one specified by the by parameter). The `aggfunc` can be a function, a string, or a dictionary mapping column names to operations. Common aggregation functions include:\n",
    "\n",
    "* '`sum`': Sum of values.\n",
    "* '`mean`': Mean of values.\n",
    "* '`max`': Maximum value.\n",
    "* '`min`': Minimum value.\n",
    "* '`median`': Median of values.\n",
    "* '`std`': Standard deviation of values.\n",
    "* '`var`': Variance of values.\n",
    "* '`first`': First value.\n",
    "* '`last`': Last value.\n",
    "* '`count`': Count of non-null values.\n",
    "\n",
    "You can also use any function that is accepted by the aggregate method of pandas DataFrame. This includes functions from the numpy library like numpy.sum or numpy.mean, and you can also define your own custom aggregation functions.\n",
    "\n",
    "If you need to apply different aggregation functions to different columns, you can pass a dictionary to aggfunc, where keys are column names and values are functions or names of functions. For example:\n",
    "\n",
    "`aggfunc={'population': 'sum', 'area': 'mean'}`\n",
    "\n",
    "This would sum the values in the 'population' column and calculate the mean of the values in the 'area' column for each group.\n",
    "\n",
    "It's important to note that the aggregation function(s) you choose should make sense for the type of data you're working with and the specific analysis or outcome you're aiming for. For instance, summing up categorical data or taking the mean of ordinal data might not yield meaningful results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
