{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rasterio\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import box\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "from rasterio import features\n",
    "\n",
    "from rasterio.features import rasterize\n",
    "from rasterio.transform import from_origin\n",
    "import pandas as pd\n",
    "from rasterio import features\n",
    "from shapely.geometry import Polygon, MultiPolygon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCTION \n",
    "\n",
    "The raster_to_polygons_optimized function is designed to convert a raster dataset into a GeoDataFrame of polygons, where each polygon represents an individual cell from the raster. \n",
    "\n",
    "The function takes a single argument, raster_path, which specifies the file path of the raster to be processed. It begins by opening the raster file using rasterio, extracting the transformation parameters, reading the first band of data, and identifying the no-data value specified in the raster's metadata. \n",
    "\n",
    "The function then identifies all valid cells—those not equal to the no-data value—and iterates over these cells. For each valid cell, it calculates the polygon geometry that corresponds to the cell's spatial extent using the raster's transformation parameters and assigns the cell's value to this polygon. \n",
    "\n",
    "These polygons and their associated values are then aggregated into a GeoDataFrame, which is assigned the same Coordinate Reference System (CRS) as the source raster. \n",
    "\n",
    "The resulting GeoDataFrame, containing a 'value' column for cell values and a 'geometry' column for the corresponding polygons, is returned by the function. This optimized approach ensures efficient processing by focusing only on valid raster cells, thereby excluding areas with no-data values and reducing computational overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raster_to_polygons_optimized(raster_path):\n",
    "    \"\"\"\n",
    "    Convert a raster dataset to a GeoDataFrame where each valid cell (non-no-data) is represented as a polygon.\n",
    "\n",
    "    This function opens a raster file, reads its first band, and converts each valid cell into a polygon. The value\n",
    "    of each cell is retained and associated with the corresponding polygon. The resulting GeoDataFrame contains\n",
    "    polygons with their associated values and is set to the same CRS as the input raster.\n",
    "\n",
    "    Parameters:\n",
    "    - raster_path (str): The file path of the raster to be processed.\n",
    "\n",
    "    Returns:\n",
    "    - gpd.GeoDataFrame: A GeoDataFrame with two columns: 'value' containing the cell values and 'geometry'\n",
    "      containing the corresponding polygon geometries for each valid raster cell.\n",
    "    \"\"\"\n",
    "    # Extract the base filename without extension to use as the column name\n",
    "    filename = os.path.splitext(os.path.basename(raster_path))[0]\n",
    "\n",
    "    # Open the raster file using rasterio\n",
    "    with rasterio.open(raster_path) as src:\n",
    "        \n",
    "        # Extract the affine transform for the raster\n",
    "        transform = src.transform\n",
    "        # Read the first band of the raster\n",
    "        data = src.read(1)\n",
    "        # Retrieve the no-data value set in the raster's metadata\n",
    "        nodata = src.nodata\n",
    "\n",
    "    # Identify valid cells (cells not equal to the no-data value)\n",
    "    valid_cells = np.argwhere(data != nodata)\n",
    "\n",
    "    polygons = []  # List to store polygon geometries\n",
    "    values = []  # List to store the values of the valid cells\n",
    "\n",
    "    # Iterate over the indices of valid cells to create polygons\n",
    "    for (j, i) in valid_cells:\n",
    "        # Extract the value of the current cell\n",
    "        value = data[j, i]\n",
    "        # Create a polygon geometry for the current cell based on its coordinates and the raster's transform\n",
    "        polygon = box(\n",
    "            transform[2] + i * transform[0],  # Minx\n",
    "            transform[5] + (j + 1) * transform[4],  # Miny\n",
    "            transform[2] + (i + 1) * transform[0],  # Maxx\n",
    "            transform[5] + j * transform[4]  # Maxy\n",
    "        )\n",
    "        # Append the current polygon and its value to their respective lists\n",
    "        polygons.append(polygon)\n",
    "        values.append(value)\n",
    "\n",
    "    # Create a GeoDataFrame with the polygons and their associated values\n",
    "    gdf = gpd.GeoDataFrame({filename: values, 'geometry': polygons})\n",
    "    # Set the CRS of the GeoDataFrame to match the CRS of the source raster\n",
    "    gdf.crs = src.crs\n",
    "\n",
    "    return gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gdf_test = raster_to_polygons_optimized()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A raster transform is a set of coefficients that define the spatial relationship between the coordinates of pixels in a raster dataset and the geographic coordinates on the surface of the Earth. This transform allows you to map the row and column indices of pixels in the raster grid to their corresponding geographic locations (such as latitude and longitude or coordinates in a projected coordinate system).\n",
    "\n",
    "The raster transform typically includes six coefficients, often represented in an affine transformation matrix. These coefficients are:\n",
    "\n",
    "a: The width of a pixel in the units of the coordinate system (e.g., meters).\n",
    "b and d: These coefficients typically rotate the raster, but in most north-up images, these are zero.\n",
    "c: The X-coordinate of the upper-left corner of the upper-left pixel.\n",
    "e: The height of a pixel in the units of the coordinate system, which is usually a negative value because pixel row indices increase downward, while geographic coordinates usually increase upward.\n",
    "f: The Y-coordinate of the upper-left corner of the upper-left pixel.\n",
    "In the context of the rasterio library, the transform is often used to calculate the geographic coordinates of the center of a pixel given its row and column indices, or vice versa. The transform object can perform these calculations through methods like * (to convert pixel coordinates to geographic coordinates) and ~ (to convert geographic coordinates to pixel coordinates).\n",
    "\n",
    "An example affine transform could look like this: [a, b, c, d, e, f], where a, d, and e are the pixel sizes and rotation terms, and c and f give the geographic coordinates of the upper-left pixel.\n",
    "\n",
    "Understanding the raster transform is crucial for tasks like georeferencing, where you need to align the raster data with a specific geographic location, or when performing spatial analyses that require the conversion of pixel coordinates to real-world geographic locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Raster(s) and Create 'INPUT REPORTING UNITS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest resolution: (97.38204323985136, 99.43096076870454)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Folder containing all your raster files\n",
    "raster_folder = r\"C:\\Users\\bsf31\\Documents\\post-meds\\data\\signal\\Moore\\built_model\\rasters\"\n",
    "# Get a list of all raster files in the folder (assuming they are TIFF files)\n",
    "raster_files = [f for f in os.listdir(raster_folder) if f.endswith('.tif')]\n",
    "\n",
    "# Initialize variables to store the smallest resolution\n",
    "smallest_res = None\n",
    "\n",
    "# Iterate through each raster file to find the smallest resolution\n",
    "for raster_file in raster_files:\n",
    "    with rasterio.open(os.path.join(raster_folder, raster_file)) as src:\n",
    "        # src.res returns a tuple (width, height) of pixels\n",
    "        if smallest_res is None or (src.res[0] < smallest_res[0] and src.res[1] < smallest_res[1]):\n",
    "            smallest_res = src.res\n",
    "\n",
    "# smallest_res now contains the smallest resolution by width and height\n",
    "print(\"Smallest resolution:\", smallest_res)\n",
    "\n",
    "# Initialize an empty GeoDataFrame to store the combined results\n",
    "combined_gdf = gpd.GeoDataFrame()\n",
    "\n",
    "# Process each raster file\n",
    "for raster_file in raster_files:\n",
    "    raster_path = os.path.join(raster_folder, raster_file)\n",
    "    with rasterio.open(raster_path) as src:\n",
    "        # Check if the raster's resolution matches the most common resolution\n",
    "        if src.res != smallest_res:\n",
    "            print(f\"Skipping {raster_file} due to mismatching resolution.\")\n",
    "            continue  # Skip this file\n",
    "\n",
    "    # If resolution matches, process the raster\n",
    "    current_gdf = raster_to_polygons_optimized(raster_path)\n",
    "    \n",
    "    # Merge the current GeoDataFrame with the combined one\n",
    "    if combined_gdf.empty:\n",
    "        combined_gdf = current_gdf\n",
    "    else:\n",
    "        combined_gdf = combined_gdf.merge(current_gdf, on='geometry', how='outer')\n",
    "\n",
    "# Save the combined GeoDataFrame to a GeoPackage\n",
    "output_gpkg_path = r'C:\\Users\\bsf31\\Documents\\post-meds\\data\\signal\\Moore\\built_model\\input_reporting_units\\input_reporting_units.gpkg'\n",
    "combined_gdf.to_file(output_gpkg_path, layer='built_env', driver='GPKG')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\bsf31\\\\Documents\\\\post-meds\\\\data\\\\signal\\\\Moore\\\\built_model\\\\input_reporting_units\\\\input_reporting_units.gpkg'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_gpkg_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional - Append New Single File Raster Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the existing GeoPackage and the new raster file\n",
    "existing_gpkg_path = output_gpkg_path\n",
    "# Load the existing GeoPackage into a GeoDataFrame\n",
    "existing_gdf = gpd.read_file(existing_gpkg_path, layer='combined_layer')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_raster_path = 'path_to_new_raster_file.tif'\n",
    "# Process the new raster to a GeoDataFrame\n",
    "new_gdf = raster_to_polygons_optimized(new_raster_path)\n",
    "\n",
    "# Merge the new GeoDataFrame with the existing one\n",
    "# Ensure the merge is done based on geometry or another suitable common attribute\n",
    "updated_gdf = existing_gdf.merge(new_gdf, on='geometry', how='outer')\n",
    "\n",
    "# Update the GeoPackage with the updated GeoDataFrame\n",
    "# This example overwrites the existing layer; you could also choose to add a new layer\n",
    "updated_gdf.to_file(existing_gpkg_path, layer='combined_layer', driver='GPKG')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_path = r\"C:\\Users\\bsf31\\Documents\\post-meds\\data\\signal\\RWMP_AREA_2229.gpkg\"\n",
    "study_area = gpd.read_file(study_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REFERENCE RASTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_crs = 'EPSG:2229'\n",
    "output_resolution = (97.37954817629196214, 99.34198426966294448)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97.37954817629196, -99.34198426966294)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5807047.11341745, 1961361.28632066, 6127434.03567656,\n",
       "       2040906.05493562])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study_area_bounds =study_area.total_bounds\n",
    "study_area_bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector to Raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_to_raster(input_vector, output_raster, attribute, study_area_bounds, value_mapping, single_value=None, resolution=(abs(0.00026949458523585647), abs(-0.00026949458523585647)), dtype='uint16'):\n",
    "    # Check if input_vector is a GeoDataFrame or a file path\n",
    "    if isinstance(input_vector, gpd.GeoDataFrame):\n",
    "        gdf = input_vector\n",
    "    else:\n",
    "        # Read the input vector file (GeoPackage or Shapefile) into a GeoDataFrame\n",
    "        gdf = gpd.read_file(input_vector)\n",
    "    # Reproject the GeoDataFrame to the desired CRS (EPSG:2229)\n",
    "    gdf = gdf.to_crs(epsg=2229)\n",
    "\n",
    "    # Ensure that categorical column is string\n",
    "    gdf[attribute] = gdf[attribute].astype(str)\n",
    "   \n",
    "\n",
    "   # If single_value is None, convert the attribute column to numerical values\n",
    "    # If single_value is provided, set the attribute column to the provided single_value\n",
    "    if single_value is None:\n",
    "        gdf[attribute] = gdf[attribute].replace(value_mapping).astype(dtype)\n",
    "    else:\n",
    "        gdf[attribute] = single_value\n",
    "\n",
    "\n",
    "    # Use the study area bounds and resolution to calculate the width and height of the output raster\n",
    "    minx, miny, maxx, maxy = study_area_bounds\n",
    "    width = int((maxx - minx) / resolution[0])\n",
    "    height = int((maxy - miny) / resolution[1])\n",
    "\n",
    "    out_transform = rasterio.transform.from_bounds(minx, miny, maxx, maxy, width, height)\n",
    "\n",
    "\n",
    "    # Define the metadata for the output raster file\n",
    "    out_meta = {\n",
    "        'driver': 'GTiff',\n",
    "        'width': width,\n",
    "        'height': height,\n",
    "        'count': 1,\n",
    "        'dtype': dtype,\n",
    "        'crs': 'EPSG:2229',\n",
    "        'transform': out_transform,\n",
    "        'nodata': 0\n",
    "    }\n",
    "    \n",
    "    # Open the output raster file for writing with the specified metadata\n",
    "    with rasterio.open(output_raster, 'w', **out_meta) as dst:\n",
    "        # Create a generator of tuples containing the geometry and attribute value for each feature in the input vector data\n",
    "        shapes = ((geom, value) for geom, value in zip(gdf['geometry'], gdf[attribute]))\n",
    "        \n",
    "        # Burn the geometries and their corresponding attribute values into a raster array\n",
    "        burned = features.rasterize(\n",
    "            shapes=shapes,         # The generator of geometry-attribute tuples\n",
    "            fill=0,                # The default value for pixels not covered by any geometry\n",
    "            out_shape=(height, width), # The shape of the output raster array (number of rows and columns)\n",
    "            transform=out_transform,   # The affine transformation matrix that maps pixel coordinates to the coordinate reference system\n",
    "            dtype=dtype            # The data type of the raster array\n",
    "        )\n",
    "        \n",
    "        # Write the burned raster array to the output raster file\n",
    "        dst.write(burned, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to call Vector to Raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To write text file of value mapping\n",
    "def write_value_mapping(value_mapping, output_file):\n",
    "    with open(output_file, 'w') as f:\n",
    "        for key, value in value_mapping.items():\n",
    "            f.write(f'{key}: {value}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_columns(input_vector, output_dir, study_area_bounds, resolution, columns, value_mapping, single_value=None, file_name=None):\n",
    "    if file_name is None:\n",
    "        file_name = os.path.splitext(os.path.basename(input_vector))[0]\n",
    "\n",
    "    for column in columns:\n",
    "        column_output_dir = os.path.join(output_dir, column)\n",
    "        os.makedirs(column_output_dir, exist_ok=True)\n",
    "\n",
    "        output_raster = f\"{column_output_dir}/{file_name}_{column}_raster.tif\"\n",
    "        vector_to_raster(input_vector, output_raster, column, study_area_bounds, value_mapping[column], single_value=single_value, resolution=resolution)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files_from_list(file_list, output_dir, study_area_bounds, resolution, columns, single_value=None):\n",
    "    # Generate a global value mapping from the list of unique values for each column\n",
    "    global_value_mapping = {column: set() for column in columns}\n",
    "    \n",
    "    for file_path in file_list:\n",
    "        gdf = gpd.read_file(file_path)\n",
    "        for column in columns:\n",
    "            global_value_mapping[column].update(gdf[column].astype(str).unique())\n",
    "    \n",
    "    for column in columns:\n",
    "        global_value_mapping[column] = {value: idx for idx, value in enumerate(sorted(list(global_value_mapping[column])), 1)}\n",
    "        print(f\"Global value mapping for {column}:\")\n",
    "        for value in global_value_mapping[column]:\n",
    "            print(f\"{value}: {global_value_mapping[column][value]}\")\n",
    "    \n",
    "    # Write the global value mapping for each column to separate .txt files in the corresponding column folder\n",
    "    for column in columns:\n",
    "        column_output_dir = os.path.join(output_dir, column)\n",
    "        os.makedirs(column_output_dir, exist_ok=True)\n",
    "\n",
    "        output_value_mapping_file = f\"{column_output_dir}/{column}_global_value_mapping.txt\"\n",
    "        write_value_mapping(global_value_mapping[column], output_value_mapping_file)\n",
    "\n",
    "    # Process each file with the global value mapping\n",
    "    for file_path in file_list:\n",
    "        file_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        process_columns(file_path, output_dir, study_area_bounds, columns=columns, resolution=resolution, value_mapping=global_value_mapping, single_value=single_value, file_name=file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where files will save, can add subfolders if desired\n",
    "output_dir = os.path.join(r\"C:\\Users\\bsf31\\Documents\\post-meds\\data\\signal\\Moore\\built_model\\base\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "vector = [os.path.join(output_dir, 'esha_riparian_rwmp_epsg2229.gpkg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global value mapping for OVERLAY:\n",
      "Environmentally Sensitive Habitat Overlay: 1\n",
      "Riparian Corridor Overlay: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bsf31\\AppData\\Local\\Temp\\ipykernel_15048\\2005158419.py:18: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  gdf[attribute] = gdf[attribute].replace(value_mapping).astype(dtype)\n"
     ]
    }
   ],
   "source": [
    "process_files_from_list(vector, output_dir, study_area_bounds, output_resolution, columns=['OVERLAY'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where files will save, can add subfolders if desired\n",
    "output_dir = os.path.join(r\"C:\\Users\\bsf31\\Documents\\post-meds\\data\\signal\\Moore\\built_model\\base\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "vector = [os.path.join(output_dir, 'ag_greenbelts_02182021.gpkg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global value mapping for Id:\n",
      "0: 1\n"
     ]
    }
   ],
   "source": [
    "process_files_from_list(vector, output_dir, study_area_bounds, output_resolution, columns=['Id'], single_value=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where files will save, can add subfolders if desired\n",
    "output_dir = os.path.join(r\"C:\\Users\\bsf31\\Documents\\post-meds\\data\\signal\\Moore\\built_model\\base\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "vector = [os.path.join(output_dir, 'lu_catagories.gpkg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global value mapping for Category:\n",
      "Agricultural: 1\n",
      "Commercial: 2\n",
      "Critical Infrastructure: 3\n",
      "High-Risk Infrastructure: 4\n",
      "Miscellaneous: 5\n",
      "Null: 6\n",
      "Residential: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bsf31\\AppData\\Local\\Temp\\ipykernel_15048\\2005158419.py:18: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  gdf[attribute] = gdf[attribute].replace(value_mapping).astype(dtype)\n"
     ]
    }
   ],
   "source": [
    "process_files_from_list(vector, output_dir, study_area_bounds, output_resolution, columns=['Category'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_reporting_units = gpd.read_file(r\"C:\\Users\\bsf31\\Documents\\post-meds\\data\\signal\\Moore\\built_model\\input_reporting_units\\input_reporting_units.gpkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ag_greenbelts_02182021_Id_raster',\n",
       "       'esha_riparian_rwmp_epsg2229_OVERLAY_raster',\n",
       "       'lu_catagories_Category_raster', 'geometry'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_reporting_units.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: The 'ag_greenbelts_02182021_Id_raster' column contains missing values.\n",
      "Warning: The 'esha_riparian_rwmp_epsg2229_OVERLAY_raster' column contains missing values.\n",
      "Warning: The 'lu_catagories_Category_raster' column contains missing values.\n",
      "Input reporting units file updated with null values fixed. Updated file saved to: C:\\Users\\bsf31\\Documents\\post-meds\\data\\signal\\Moore\\built_model\\input_reporting_units\\input_reporting_units_updated.gpkg\n"
     ]
    }
   ],
   "source": [
    "columns_to_check = ['ag_greenbelts_02182021_Id_raster', 'esha_riparian_rwmp_epsg2229_OVERLAY_raster', 'lu_catagories_Category_raster' ]  # Add more column names if needed\n",
    "\n",
    "for column in columns_to_check:\n",
    "    if input_reporting_units[column].isnull().any():\n",
    "        print(f\"Warning: The '{column}' column contains missing values.\")\n",
    "        # Replace missing values with an appropriate value based on your requirements\n",
    "        input_reporting_units[column] = input_reporting_units[column].fillna(0) \n",
    "    else:\n",
    "        print(f\"No missing values found in the '{column}' column.\")\n",
    "\n",
    "# Save the updated input reporting units file\n",
    "output_file = r'C:\\Users\\bsf31\\Documents\\post-meds\\data\\signal\\Moore\\built_model\\input_reporting_units\\input_reporting_units_updated.gpkg'\n",
    "input_reporting_units.to_file(output_file, driver='GPKG')\n",
    "\n",
    "print(\"Input reporting units file updated with null values fixed. Updated file saved to:\", output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
